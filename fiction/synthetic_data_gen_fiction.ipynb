{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "941dc796-3af4-4736-a867-aee98b21a9c5",
   "metadata": {},
   "source": [
    "# Generate synthetic data for fiction\n",
    "\n",
    "Uses the OpenAI API to generate paraphases and summaries of fiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bd9039b-e46a-4604-90d9-e6d10d4025b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textwrap\n",
    "import random\n",
    "\n",
    "import backoff\n",
    "import openai\n",
    "\n",
    "def print_wrapped_text(text, width=70):\n",
    "    texts = text.split('\\n')\n",
    "    if len(texts) > 1:\n",
    "        for t in texts:\n",
    "            print_wrapped_text(t, width=70)\n",
    "\n",
    "    else:\n",
    "        text = texts[0]\n",
    "        wrapper = textwrap.TextWrapper(width=width)\n",
    "        wrapped_text = wrapper.fill(text)\n",
    "        print(wrapped_text)\n",
    "    \n",
    "with open('credentials.txt', encoding = 'utf-8') as f:\n",
    "    organization = f.readline().strip()\n",
    "    api_key = f.readline().strip()\n",
    "    \n",
    "client = openai.OpenAI(organization=organization, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1c3b33-c399-44ac-8750-ff8f368e0187",
   "metadata": {},
   "source": [
    "#### Load the file that needs augmenting\n",
    "\n",
    "The data we're about to load was created locally. It's very large and we can't realistically generate that much, so let's sample a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1c2ef95-a936-4f3c-b7a3-b810c34c71ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sv/vq7zp61n041cl5pvhlrg3hkm0000gp/T/ipykernel_36563/598152692.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  meta = pd.read_csv('training_pairs_fiction.tsv', sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>year</th>\n",
       "      <th>anchor</th>\n",
       "      <th>positive</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uc2.ark+=13960=t4jm23x05</td>\n",
       "      <td>1896</td>\n",
       "      <td>Over the pure white limestone and shells of th...</td>\n",
       "      <td>There was rigged on the stern of the boat a pr...</td>\n",
       "      <td>successive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20951</td>\n",
       "      <td>1971</td>\n",
       "      <td>He plucked Ulysses’ infant son from his nurse’...</td>\n",
       "      <td>And that is why we must all go now to the aid ...</td>\n",
       "      <td>successive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11252</td>\n",
       "      <td>1929</td>\n",
       "      <td>Were you able to connect him with that too?” “...</td>\n",
       "      <td>The murderer, secure in his conviction that su...</td>\n",
       "      <td>successive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nyp.33433082304480</td>\n",
       "      <td>1915</td>\n",
       "      <td>But the Regent refused to listen to them, and ...</td>\n",
       "      <td>But there were very few who felt and thought l...</td>\n",
       "      <td>successive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nyp.33433074863048</td>\n",
       "      <td>1908</td>\n",
       "      <td>Your little pupils, too, I have met — \" Mr. Nu...</td>\n",
       "      <td>It did not take her a second to do the sum — b...</td>\n",
       "      <td>synthetic-pair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    paperId  year  \\\n",
       "0  uc2.ark+=13960=t4jm23x05  1896   \n",
       "1                     20951  1971   \n",
       "2                     11252  1929   \n",
       "3        nyp.33433082304480  1915   \n",
       "4        nyp.33433074863048  1908   \n",
       "\n",
       "                                              anchor  \\\n",
       "0  Over the pure white limestone and shells of th...   \n",
       "1  He plucked Ulysses’ infant son from his nurse’...   \n",
       "2  Were you able to connect him with that too?” “...   \n",
       "3  But the Regent refused to listen to them, and ...   \n",
       "4  Your little pupils, too, I have met — \" Mr. Nu...   \n",
       "\n",
       "                                            positive        category  \n",
       "0  There was rigged on the stern of the boat a pr...      successive  \n",
       "1  And that is why we must all go now to the aid ...      successive  \n",
       "2  The murderer, secure in his conviction that su...      successive  \n",
       "3  But there were very few who felt and thought l...      successive  \n",
       "4  It did not take her a second to do the sum — b...  synthetic-pair  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.read_csv('training_pairs_fiction.tsv', sep='\\t')\n",
    "meta = meta.sample(150000).reset_index(drop=True)\n",
    "print(meta.shape)\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4664c",
   "metadata": {},
   "source": [
    "## The possible categories\n",
    "\n",
    "single-synthetic: a single chunk paired with the word \"generate,\" which is a signal to generate a\n",
    "        paraphrase of the chunk using an LLM. 8% of all pairs.\n",
    "\n",
    "successive: two successive chunks from the same article. 88% of all pairs.\n",
    "        \n",
    "synthetic-pair: a successive pair of chunks from the same article, of which one should be replaced\n",
    "        by a synthetic paraphrase. Only 2% of all pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "628a5a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>successive</td>\n",
       "      <td>135911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>synthetic-pair</td>\n",
       "      <td>7103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>single-synthetic</td>\n",
       "      <td>6986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Category   Count\n",
       "0        successive  135911\n",
       "1    synthetic-pair    7103\n",
       "2  single-synthetic    6986"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts = meta['category'].value_counts()\n",
    "category_counts_table = pd.DataFrame({'Category': category_counts.index, 'Count': category_counts.values})\n",
    "category_counts_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7105044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide meta into a list of dataframes, each 15,000 rows long\n",
    "# so we can do this in stages and don't lose everything if the connection drops\n",
    "\n",
    "meta_list = [meta[i:i+15000] for i in range(0, meta.shape[0], 15000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d839816-33a0-4eb1-a68a-1b62ca52d2e4",
   "metadata": {},
   "source": [
    "### The function that actually calls the API\n",
    "\n",
    "We surround this with ```backoff``` instructions to handle errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d30026c-0ebd-49d7-9212-8657bc9fe704",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    openai.RateLimitError,\n",
    "    max_time=60,  # Set a maximum wait time in seconds (adjust as needed)\n",
    "    giveup=lambda e: False  # This prevents giving up on retries\n",
    ")\n",
    "def completions_with_backoff(**kwargs):\n",
    "    global client\n",
    "    try:\n",
    "        return client.chat.completions.create(**kwargs)\n",
    "    except openai.APIError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise  # Re-raise the error to trigger the retry mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e15bc8-1304-44b1-a6aa-3a9199e75bf7",
   "metadata": {},
   "source": [
    "#### System prompts\n",
    "\n",
    "We have five different system prompts to create slightly different flavors of summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4231fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_A = \"You are a helpful editor condensing passages of fiction. Your goal is to \\\n",
    "perserve the meaning of the original text while reducing its length to four or five sentences. \\\n",
    "Omit details as necessary. Don't add any editorial comments or questions. \\\n",
    "Reply only with your rephrased summary of the passage.\"\n",
    "\n",
    "system_prompt_B = \"You are a helpful editor condensing passages of fiction. Your goal is to \\\n",
    "preserve the meaning of the text while reducing its length to two or three sentences. \\\n",
    "Omit details as necessary, and condense dialogue. Don't add any editorial comments or questions. \\\n",
    "Reply only with your rephrased summary of the passage.\"\n",
    "\n",
    "system_prompt_C = \"You are an editor who disguises the identity of characters in fiction. When \\\n",
    "given a passage of fiction, you give three numbered replies. \\n\\\n",
    "1. A single personal name in the passage that you select to be changed--or 'none' if the passage does not contain names. \\n\\\n",
    "2. A new invented name that will replace it (or 'none' if 1 was 'none'). \\n\\\n",
    "3. The original passage, with all instances of the first name replaced by the second. \\\n",
    "If the passage does not contain personal names, this will be the original passage unchanged.\"\n",
    "\n",
    "user_C = \"Lisa and Stanley discuss the newcomer Todd Graham, who took over Alistair Hubbard's business. \\\n",
    "Lisa questions Graham's belonging in their community, prompting Stanley to agree that Graham doesn't fit the old-fashioned charm of \\\n",
    "Bellville. Stanley recalls a conversation hinting at foul play regarding Hubbard's death and subtly asks Lisa about the situation.\"\n",
    "\n",
    "assistant_C = \"1. Lisa. \\n\\\n",
    "2. Karen. \\n\\\n",
    "3. Karen and Stanley discuss the newcomer Todd Graham, who took over Alistair Hubbard's business. \\\n",
    "Karen questions Graham's belonging in their community, prompting Stanley to agree that Graham doesn't fit the old-fashioned charm of \\\n",
    "Bellville. Stanley recalls a conversation hinting at foul play regarding Hubbard's death and subtly asks Karen about the situation.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b605811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Lisa.\n",
      "2. Karen.\n",
      "3. Karen and Stanley discuss the newcomer Todd Graham, who took over\n",
      "Alistair Hubbard's business. Karen questions Graham's belonging in\n",
      "their community, prompting Stanley to agree that Graham doesn't fit\n",
      "the old-fashioned charm of Bellville. Stanley recalls a conversation\n",
      "hinting at foul play regarding Hubbard's death and subtly asks Karen\n",
      "about the situation.\n"
     ]
    }
   ],
   "source": [
    "print_wrapped_text(assistant_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "788e2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_prompt(system_prompt, user1, temperature):\n",
    "    \n",
    "    prompt = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "              {\"role\": \"user\", \"content\": user1}]\n",
    "\n",
    "    p = list(prompt)\n",
    "    # print(p)\n",
    "    try:\n",
    "        completion = completions_with_backoff(\n",
    "            model = \"gpt-3.5-turbo\",\n",
    "            messages = p,\n",
    "            max_tokens = 768,\n",
    "            temperature = temperature\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08f1d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_complex_prompt(system_prompt, user2, assistant1, user3, temperature):\n",
    "    \n",
    "    prompt = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "              {\"role\": \"user\", \"content\": user2},\n",
    "              {\"role\": \"assistant\", \"content\": assistant1},\n",
    "              {\"role\": \"user\", \"content\": user3}]\n",
    "\n",
    "    p = list(prompt)\n",
    "    # print(p)\n",
    "    try:\n",
    "        completion = completions_with_backoff(\n",
    "            model = \"gpt-3.5-turbo\",\n",
    "            messages = p,\n",
    "            max_tokens = 1024,\n",
    "            temperature = temperature\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        \n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4604f",
   "metadata": {},
   "source": [
    "## Actually generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0a56eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk of meta: 6\n",
      "500 15206 0.536\n",
      "1000 30550 0.592\n",
      "1500 45015 0.506\n",
      "2000 64728 0.51\n",
      "2500 82410 0.467\n",
      "3000 98504 0.497\n",
      "3500 113037 0.5\n",
      "4000 127925 0.511\n",
      "4500 141593 0.481\n",
      "5000 158879 0.487\n",
      "5500 177388 0.479\n",
      "6000 195011 0.471\n",
      "6500 209628 0.47\n",
      "7000 225816 0.469\n",
      "7500 240994 0.475\n",
      "8000 254928 0.473\n",
      "8500 270414 0.475\n",
      "9000 284942 0.468\n",
      "9500 296461 0.471\n",
      "10000 311531 0.472\n",
      "10500 326631 0.482\n",
      "11000 345367 0.481\n",
      "11500 360678 0.48\n",
      "12000 375531 0.477\n",
      "12500 390330 0.471\n",
      "13000 406090 0.474\n",
      "13500 420764 0.472\n",
      "14000 437682 0.475\n",
      "14500 451453 0.479\n",
      "15000 473348 0.477\n",
      "Word count: 473348\n",
      "Chunk of meta: 7\n",
      "500 14853 0.481\n",
      "1000 30806 0.482\n",
      "1500 50037 0.487\n",
      "2000 65372 0.488\n",
      "2500 78727 0.489\n",
      "3000 92483 0.487\n",
      "3500 113650 0.49\n",
      "4000 130047 0.493\n",
      "4500 143726 0.493\n",
      "5000 160055 0.497\n",
      "5500 172269 0.496\n",
      "6000 185064 0.494\n",
      "6500 198011 0.5\n",
      "7000 211934 0.503\n",
      "7500 234102 0.503\n",
      "8000 251199 0.502\n",
      "8500 266939 0.503\n",
      "9000 282099 0.505\n",
      "9500 297304 0.504\n",
      "10000 315414 0.502\n",
      "10500 331172 0.5\n",
      "11000 346799 0.502\n",
      "11500 363847 0.502\n",
      "12000 378078 0.5\n",
      "12500 396131 0.504\n",
      "13000 413448 0.504\n",
      "13500 427576 0.504\n",
      "14000 445300 0.503\n",
      "14500 462436 0.505\n",
      "15000 472742 0.504\n",
      "Word count: 472742\n",
      "Chunk of meta: 8\n",
      "500 12507 0.506\n",
      "1000 27910 0.505\n",
      "1500 45487 0.504\n",
      "2000 61516 0.499\n",
      "2500 74662 0.497\n",
      "3000 88779 0.499\n",
      "3500 101390 0.5\n",
      "4000 120078 0.5\n",
      "4500 131701 0.498\n",
      "5000 147335 0.496\n",
      "5500 164385 0.494\n",
      "6000 178606 0.494\n",
      "6500 193242 0.494\n",
      "7000 203521 0.494\n",
      "7500 215881 0.495\n",
      "8000 230539 0.494\n",
      "8500 249416 0.494\n",
      "9000 264485 0.496\n",
      "9500 282044 0.498\n",
      "10000 297010 0.499\n",
      "10500 311558 0.498\n",
      "11000 330629 0.497\n",
      "11500 349411 0.497\n",
      "12000 367297 0.499\n",
      "12500 381554 0.5\n",
      "13000 400282 0.501\n",
      "13500 418845 0.502\n",
      "14000 436080 0.502\n",
      "14500 456632 0.503\n",
      "15000 471704 0.505\n",
      "Word count: 471704\n",
      "Chunk of meta: 9\n",
      "500 14791 0.506\n",
      "1000 31601 0.507\n",
      "1500 47723 0.508\n",
      "2000 64646 0.508\n",
      "2500 77665 0.506\n",
      "3000 90552 0.505\n",
      "3500 106631 0.505\n",
      "4000 121437 0.506\n",
      "4500 137431 0.505\n",
      "5000 155559 0.505\n",
      "5500 169223 0.505\n",
      "6000 183061 0.505\n",
      "6500 195682 0.504\n",
      "7000 213445 0.506\n",
      "7500 231056 0.504\n",
      "8000 245772 0.503\n",
      "8500 258356 0.502\n",
      "9000 273594 0.501\n",
      "9500 290398 0.502\n",
      "10000 304358 0.503\n",
      "10500 319855 0.504\n",
      "11000 334921 0.505\n",
      "11500 348392 0.505\n",
      "12000 367558 0.506\n",
      "12500 384080 0.507\n",
      "13000 400519 0.506\n",
      "13500 418407 0.506\n",
      "14000 433638 0.507\n",
      "14500 447464 0.507\n",
      "15000 462660 0.507\n",
      "Word count: 462660\n"
     ]
    }
   ],
   "source": [
    "# all_new_dfs = []\n",
    "successes = 1\n",
    "failures = 0\n",
    "\n",
    "for meta_idx, meta in enumerate(meta_list[6: ]):\n",
    "    print('Chunk of meta:', meta_idx + 6)\n",
    "    ctr = 0\n",
    "\n",
    "    # we save rows as a list of dictionaries, where keys are\n",
    "    # paperId, year, anchor, positive, system_prompt, category, and temperature\n",
    "    # system_prompt and temperature will be blank for rows that\n",
    "    # do not require synthetic data\n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    wordcount = 0\n",
    "\n",
    "    for idx, row in meta.iterrows():\n",
    "\n",
    "        ctr += 1\n",
    "        if ctr % 500 == 0:\n",
    "            print(ctr, wordcount, round(successes / (successes + failures), 3))\n",
    "        \n",
    "        category = row['category']\n",
    "        year = row['year']\n",
    "        \n",
    "        if category == 'successive':\n",
    "            new_row = {'paperId': row['paperId'], 'year': year, 'anchor': row['anchor'], \n",
    "                    'positive': row['positive'], 'category': category, 'system_prompt': '', 'temperature': ''}\n",
    "            new_rows.append(new_row)\n",
    "            continue    # we don't need to generate synthetic data for these categories\n",
    "        \n",
    "        if row['anchor'] == 'generate':\n",
    "            to_replace = 'anchor'\n",
    "        elif row['positive'] == 'generate':\n",
    "            to_replace = 'positive'\n",
    "        elif category == 'synthetic-pair':\n",
    "            to_replace = random.choices(['anchor', 'positive'], weights=[0.5, 0.5])[0]\n",
    "        else:\n",
    "            print('error')\n",
    "            continue\n",
    "\n",
    "        system_prompt = random.choices([system_prompt_A, system_prompt_B], weights=[0.5, 0.5])[0]\n",
    "        \n",
    "        if system_prompt == system_prompt_A:\n",
    "            promptname = 'A'\n",
    "        elif system_prompt == system_prompt_B:\n",
    "            promptname = 'B'\n",
    "        else: \n",
    "            promptname = 'C'    \n",
    "\n",
    "        # The single-synthetic category is where we replace either the anchor or the positive\n",
    "        # with a synthetic summary of the *other* cell, producing a pair where one element\n",
    "        # is a direct paraphrase of the other.\n",
    "        \n",
    "        # The synthetic-pair category is where we replace one of the cells with a synthetic summary\n",
    "        # of itself, producing a pair of passages adjacent in the original article, one of them\n",
    "        # paraphrased.\n",
    "\n",
    "        if category == 'single-synthetic':\n",
    "            if to_replace == 'anchor':\n",
    "                user1 = row['positive']   # the text to be paraphrased comes from the non-replaced cell\n",
    "            else:\n",
    "                user1 = row['anchor']\n",
    "        elif category == 'synthetic-pair':\n",
    "            if to_replace == 'anchor':\n",
    "                user1 = row['anchor'] # the text to be paraphrased comes from the replaced cell\n",
    "            else:\n",
    "                user1 = row['positive']\n",
    "\n",
    "        temperature = round(random.uniform(0.5, 0.7), 3)\n",
    "        \n",
    "        try:\n",
    "            completion = submit_prompt(system_prompt, user1, temperature)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        summary = completion.choices[0].message.content\n",
    "\n",
    "        if promptname == 'A':\n",
    "            user1 = summary\n",
    "            success = True\n",
    "            try:\n",
    "                completion = submit_complex_prompt(system_prompt_C, user_C, assistant_C, user1, temperature)\n",
    "                renamed = completion.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                success = False\n",
    "        \n",
    "            if success and '3.' in renamed:\n",
    "                summary = renamed.split('3.')[1].strip().replace('\\n', ' ')\n",
    "                switcheroo = renamed.split('3.')[0].replace('\\n', ' ')\n",
    "            else:\n",
    "                success = False\n",
    "            \n",
    "            if success:\n",
    "                successes += 1\n",
    "                promptname = switcheroo\n",
    "            else:\n",
    "                failures += 1\n",
    "\n",
    "        # print_wrapped_text(row['anchor'])\n",
    "        # print('---')\n",
    "        # print_wrapped_text(row['positive'])\n",
    "        # print('---')\n",
    "        # print_wrapped_text(summary)\n",
    "        # print(to_replace, category, temperature)  \n",
    "        # dummy = input(\"Press Enter to continue...\")  \n",
    "\n",
    "        if to_replace == 'anchor':\n",
    "            new_row = {'paperId': row['paperId'], 'year': year, 'anchor': summary, \n",
    "                    'positive': row['positive'], 'system_prompt': promptname, \n",
    "                    'category': category, 'temperature': temperature}\n",
    "        else:\n",
    "            new_row = {'paperId': row['paperId'], 'year': year, 'anchor': row['anchor'], \n",
    "                    'positive': summary, 'system_prompt': promptname,\n",
    "                    'category': category, 'temperature': temperature}\n",
    "            \n",
    "        new_rows.append(new_row)\n",
    "\n",
    "        wordcount += len(summary.split())\n",
    "        wordcount += len(user1.split())\n",
    "        wordcount += len(system_prompt.split())\n",
    "\n",
    "    columns_sequence = ['paperId', 'year', 'anchor', 'positive', 'category', 'system_prompt', 'temperature']\n",
    "    new_df = pd.DataFrame(new_rows, columns=columns_sequence)\n",
    "    outfile = f'fiction_pairs_{meta_idx + 6}.tsv'\n",
    "    new_df.to_csv(outfile, sep='\\t', index=False)\n",
    "    all_new_dfs.append(new_df)\n",
    "    print(\"Word count:\", wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37a21364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all data frames vertically\n",
    "concatenated_df = pd.concat(all_new_dfs, axis=0)\n",
    "\n",
    "concatenated_df.to_csv('synth_fiction_pairs150k.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab712b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 7)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd676724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paperId', 'year', 'anchor', 'positive', 'system_prompt',\n",
       "       'temperature'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a081cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells with tab or newline characters in 'anchor' column: 0\n",
      "Number of cells with tab or newline characters in 'positive' column: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Count cells with tab or newline characters in 'anchor' column\n",
    "anchor_count = concatenated_df['anchor'].str.contains('\\t|\\n').sum()\n",
    "\n",
    "# Count cells with tab or newline characters in 'positive' column\n",
    "positive_count = concatenated_df['positive'].str.contains('\\t|\\n').sum()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Number of cells with tab or newline characters in 'anchor' column:\", anchor_count)\n",
    "print(\"Number of cells with tab or newline characters in 'positive' column:\", positive_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c940c381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'none': 135911, 'B': 7052, 'A': 5622, 'C': 1415})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "prompttype = Counter()\n",
    "\n",
    "for idx, row in concatenated_df.iterrows():\n",
    "    prompt = row['system_prompt']\n",
    "    if pd.isnull(prompt) or prompt == '':\n",
    "        prompttype['none'] += 1\n",
    "    elif prompt == 'A':\n",
    "        prompttype['A'] += 1\n",
    "    elif prompt == 'B':\n",
    "        prompttype['B'] += 1\n",
    "    else:\n",
    "        prompttype['C'] += 1\n",
    "\n",
    "prompttype\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddb92187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System Prompt</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>3861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  System Prompt  Count\n",
       "0             A   3861\n",
       "1             C    953\n",
       "2             B    745"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "older = pd.read_csv('old_final_pairs.tsv', sep='\\t')\n",
    "system_prompt_counts = older['system_prompt'].value_counts().reset_index()\n",
    "system_prompt_counts.columns = ['System Prompt', 'Count']\n",
    "system_prompt_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
