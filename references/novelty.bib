@article{barron2018individuals,
    author = {Barron, Alexander T. J. and Huang, Jenny and Spang, Rebecca L. and DeDeo, Simon},
    title = {Individuals, institutions, and innovation in the debates of the French Revolution},
    journal = {PNAS},
    volume = {115},
    number = {18},
    pages = {4607-4612},
    year = {2018},
    month = {April},
    doi = {10.1073/pnas.1717729115},
    editor = {Bassett, Danielle S.},
    note = {Edited by Danielle S. Bassett, University of Pennsylvania, Philadelphia, PA, and accepted by Editorial Board Member Michael S. Gazzaniga March 19, 2018 (received for review October 9, 2017)},
    email = {sdedeo@andrew.cmu.edu}
}

@article{berger2010positive,
    author = {Berger, Jonah and Sorensen, Alan T. and Rasmussen, Scott J.},
    title = {Positive Effects of Negative Publicity: When Negative Reviews Increase Sales},
    journal = {Marketing Science},
    volume = {29},
    number = {5},
    pages = {815-827},
    year = {2010},
    month = {September-October},
    doi = {10.1287/mksc.1090.0557},
    note = {Received: April 23, 2009; Accepted: December 01, 2009; Published Online: March 10, 2010},
    publisher = {INFORMS}
}

@article{zhang2023fragility,
    author = {Zhang, Letian and Banerjee, Mitali and Wang, Shinan and Hong, Zhuoqiao},
    title = {The fragility of artists’ reputations from 1795 to 2020},
    journal = {Proceedings of the National Academy of Sciences of the United States of America},
    volume = {120},
    number = {35},
    pages = {e2302269120},
    year = {2023},
    month = {August},
    doi = {10.1073/pnas.2302269120},
    email = {letian.lt.zhang@gmail.com},
    orcid = {https://orcid.org/0000-0002-3212-8303}
}


@inproceedings{danescu-niculescu-mizilNoCountryOld2013a,
  title = {No Country for Old Members: User Lifecycle and Linguistic Change in Online Communities},
  shorttitle = {No Country for Old Members},
  booktitle = {Proceedings of the 22nd International Conference on {{World Wide Web}}},
  author = {{Danescu-Niculescu-Mizil}, Cristian and West, Robert and Jurafsky, Dan and Leskovec, Jure and Potts, Christopher},
  year = {2013},
  month = may,
  pages = {307--318},
  publisher = {{ACM}},
  address = {{Rio de Janeiro Brazil}},
  doi = {10.1145/2488388.2488416},
  urldate = {2023-08-25},
  isbn = {978-1-4503-2035-1},
  langid = {english},
  file = {/Users/mstudio/Zotero/storage/PRFQ57A6/Danescu-Niculescu-Mizil et al. - 2013 - No country for old members user lifecycle and lin.pdf}
}

@article{ghosalNoveltyDetectionPerspective2022,
  title = {Novelty {{Detection}}: {{A Perspective}} from {{Natural Language Processing}}},
  shorttitle = {Novelty {{Detection}}},
  author = {Ghosal, Tirthankar and Saikh, Tanik and Biswas, Tameesh and Ekbal, Asif and Bhattacharyya, Pushpak},
  year = {2022},
  month = apr,
  journal = {Computational Linguistics},
  volume = {48},
  number = {1},
  pages = {77--117},
  issn = {0891-2017},
  doi = {10.1162/coli_a_00429},
  urldate = {2023-08-25},
  abstract = {The quest for new information is an inborn human trait and has always been quintessential for human survival and progress. Novelty drives curiosity, which in turn drives innovation. In Natural Language Processing (NLP), Novelty Detection refers to finding text that has some new information to offer with respect to whatever is earlier seen or known. With the exponential growth of information all across the Web, there is an accompanying menace of redundancy. A considerable portion of the Web contents are duplicates, and we need efficient mechanisms to retain new information and filter out redundant information. However, detecting redundancy at the semantic level and identifying novel text is not straightforward because the text may have less lexical overlap yet convey the same information. On top of that, non-novel/redundant information in a document may have assimilated from multiple source documents, not just one. The problem surmounts when the subject of the discourse is documents, and numerous prior documents need to be processed to ascertain the novelty/non-novelty of the current one in concern. In this work, we build upon our earlier investigations for document-level novelty detection and present a comprehensive account of our efforts toward the problem. We explore the role of pre-trained Textual Entailment (TE) models to deal with multiple source contexts and present the outcome of our current investigations. We argue that a multipremise entailment task is one close approximation toward identifying semantic-level non-novelty. Our recent approach either performs comparably or achieves significant improvement over the latest reported results on several datasets and across several related tasks (paraphrasing, plagiarism, rewrite). We critically analyze our performance with respect to the existing state of the art and show the superiority and promise of our approach for future investigations. We also present our enhanced dataset TAP-DLND 2.0 and several baselines to the community for further research on document-level novelty detection.},
  file = {/Users/mstudio/Zotero/storage/XYLZUJ7T/Ghosal et al. - 2022 - Novelty Detection A Perspective from Natural Lang.pdf}
}

@misc{sobchukComputationalThematicsComparing2023,
  title = {Computational Thematics: {{Comparing}} Algorithms for Clustering the Genres of Literary Fiction},
  shorttitle = {Computational Thematics},
  author = {Sobchuk, Oleg and {\v S}e{\c l}a, Artjoms},
  year = {2023},
  month = may,
  number = {arXiv:2305.11251},
  eprint = {2305.11251},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.11251},
  urldate = {2023-08-25},
  abstract = {What are the best methods of capturing thematic similarity between literary texts? Knowing the answer to this question would be useful for automatic clustering of book genres, or any other thematic grouping. This paper compares a variety of algorithms for unsupervised learning of thematic similarities between texts, which we call "computational thematics". These algorithms belong to three steps of analysis: text preprocessing, extraction of text features, and measuring distances between the lists of features. Each of these steps includes a variety of options. We test all the possible combinations of these options: every combination of algorithms is given a task to cluster a corpus of books belonging to four pre-tagged genres of fiction. This clustering is then validated against the "ground truth" genre labels. Such comparison of algorithms allows us to learn the best and the worst combinations for computational thematic analysis. To illustrate the sharp difference between the best and the worst methods, we then cluster 5000 random novels from the HathiTrust corpus of fiction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/mstudio/Zotero/storage/XYRW7TNH/Sobchuk and Šeļa - 2023 - Computational thematics Comparing algorithms for .pdf;/Users/mstudio/Zotero/storage/HM2Z65A7/2305.html}
}

@article{vicinanzaDeeplearningModelPrescient2023,
  title = {A Deep-Learning Model of Prescient Ideas Demonstrates That They Emerge from the Periphery},
  author = {Vicinanza, Paul and Goldberg, Amir and Srivastava, Sameer B},
  year = {2023},
  month = jan,
  journal = {PNAS Nexus},
  volume = {2},
  number = {1},
  pages = {pgac275},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgac275},
  urldate = {2023-08-25},
  abstract = {Where do prescient ideas\textemdash those that initially challenge conventional assumptions but later achieve widespread acceptance\textemdash come from? Although their outcomes in the form of technical innovation are readily observed, the underlying ideas that eventually change the world are often obscured. Here, we develop a novel method that uses deep learning to unearth the markers of prescient ideas from the language used by individuals and groups. Our language-based measure identifies prescient actors and documents that prevailing methods would fail to detect. Applying our model to corpora spanning the disparate worlds of politics, law, and business, we demonstrate that it reliably detects prescient ideas in each domain. Moreover, counter to many prevailing intuitions, prescient ideas emanate from each domain's periphery rather than its core. These findings suggest that the propensity to generate far-sighted ideas may be as much a property of contexts as of individuals.},
  file = {/Users/mstudio/Zotero/storage/KNSH3NC5/Vicinanza et al. - 2023 - A deep-learning model of prescient ideas demonstra.pdf}
}

@article{shibayama2021measuring,
  title={Measuring novelty in science with word embedding},
  author={Shibayama, Sotaro and Yin, Deyun and Matsumoto, Kuniko},
  journal={PloS one},
  volume={16},
  number={7},
  pages={e0254034},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{zhang2021measuring,
  title={Measuring the impact of novelty, bibliometric, and academic-network factors on citation count using a neural network},
  author={Zhang, Xinyuan and Xie, Qing and Song, Min},
  journal={Journal of Informetrics},
  volume={15},
  number={2},
  pages={101140},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{rajadesingan2020quick,
  title={Quick, community-specific learning: How distinctive toxicity norms are maintained in political subreddits},
  author={Rajadesingan, Ashwin and Resnick, Paul and Budak, Ceren},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={14},
  pages={557--568},
  year={2020}
}
@inproceedings{gururangan2022whose,
  title={Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection},
  author={Gururangan, Suchin and Card, Dallas and Dreier, Sarah and Gade, Emily and Wang, Leroy and Wang, Zeyu and Zettlemoyer, Luke and Smith, Noah A},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={2562--2580},
  year={2022}
}

@inproceedings{zhang2022detecting,
  title={Detecting Sequential Genre Change in Eighteenth-Century Texts},
  author={Zhang, Jinbin and Ryan, Yann Ciar{\'a}n and Rastas, Iiro and Ginter, Filip and Tolonen, Mikko and Babbar, Rohit},
  booktitle={Proceedings of the Computational Humanities Research Conference 2022},
  year={2022},
  organization={CEUR-WS. org}
}

@article{10.1371/journal.pone.0284567,
    doi = {10.1371/journal.pone.0284567},
    author = {Yin, Deyun AND Wu, Zhao AND Yokota, Kazuki AND Matsumoto, Kuniko AND Shibayama, Sotaro},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Identify novel elements of knowledge with word embedding},
    year = {2023},
    month = {06},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pone.0284567},
    pages = {1-16},
    abstract = {As novelty is a core value in science, a reliable approach to measuring the novelty of scientific documents is critical. Previous novelty measures however had a few limitations. First, the majority of previous measures are based on recombinant novelty concept, attempting to identify a novel combination of knowledge elements, but insufficient effort has been made to identify a novel element itself (element novelty). Second, most previous measures are not validated, and it is unclear what aspect of newness is measured. Third, some of the previous measures can be computed only in certain scientific fields for technical constraints. This study thus aims to provide a validated and field-universal approach to computing element novelty. We drew on machine learning to develop a word embedding model, which allows us to extract semantic information from text data. Our validation analyses suggest that our word embedding model does convey semantic information. Based on the trained word embedding, we quantified the element novelty of a document by measuring its distance from the rest of the document universe. We then carried out a questionnaire survey to obtain self-reported novelty scores from 800 scientists. We found that our element novelty measure is significantly correlated with self-reported novelty in terms of discovering and identifying new phenomena, substances, molecules, etc. and that this correlation is observed across different scientific fields.},
    number = {6},

}

@inproceedings{smith2013infectious,
  title={Infectious texts: Modeling text reuse in nineteenth-century newspapers},
  author={Smith, David A and Cordell, Ryan and Dillon, Elizabeth Maddock},
  booktitle={2013 IEEE International Conference on Big Data},
  pages={86--94},
  year={2013},
  organization={IEEE}
}