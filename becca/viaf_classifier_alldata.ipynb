{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#read in the full dataset",
   "id": "6fcaf8402ee1a249"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "#normalize the text\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize the text by converting to lowercase, preserving commas and spaces,\n",
    "    and removing unnecessary special characters.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'[^\\w\\s,]', '', text)  # Remove all characters except alphanumeric, spaces, and commas\n",
    "    return text.strip()  # Trim leading and trailing spaces\n",
    "df['normalized_author'] = df['author'].apply(normalize_text)"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#get the VIAF entries for all data\n",
    "def get_name(name):\n",
    "    params = {\n",
    "        'query' : f'local.personalNames = \"{name}\"',\n",
    "        'maximumRecords': 10,\n",
    "        'startRecord' : 1,\n",
    "        'sortKeys': 'holdingscount',\n",
    "        'httpAccept': 'application/json'\n",
    "    }\n",
    "\n",
    "    headers={'User-Agent': user_agent}\n",
    "    url = \"https://viaf.org/viaf/search\"\n",
    "\n",
    "    r = requests.get(url,params=params,headers=headers, timeout=10)\n",
    "    data = r.json()\n",
    "\n",
    "    return data"
   ],
   "id": "262514a265dfdbd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "user_agent = 'YOUR PROJECT NAME HERE'\n",
    "id_column_name = \"Name\"\n",
    "\n",
    "pause_between_req = 1\n",
    "\n",
    "use_title_reconcilation = True\n",
    "\n",
    "cache = {}"
   ],
   "id": "4b6468a9636e3fe5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "birthdates_df = df\n",
    "\n",
    "# Function to perform search and return DataFrame with search results\n",
    "def search_author(author_name):\n",
    "    # Initialize list to store search results\n",
    "    search_results = []\n",
    "\n",
    "    try:\n",
    "        data = get_name(author_name)\n",
    "        records = data['searchRetrieveResponse']['records']\n",
    "        total_records = len(records)\n",
    "\n",
    "        for idx, record in enumerate(records):\n",
    "            record_data = record['record']['recordData']\n",
    "            birthdate = record['record']['recordData']['birthDate']\n",
    "            viaf_title_list = []\n",
    "            # Extract titles if available\n",
    "            if 'titles' in record_data:\n",
    "                titles_data = record_data['titles']\n",
    "                if titles_data is not None:\n",
    "                    if isinstance(titles_data['work'], list):\n",
    "                        # Extract titles from the list of works\n",
    "                        viaf_title_list.extend([work['title'] for work in titles_data['work'] if 'title' in work])\n",
    "                    else:\n",
    "                        # Extract title from a single work\n",
    "                        title = titles_data['work'].get('title')\n",
    "                        if title:\n",
    "                            viaf_title_list.append(title)\n",
    "\n",
    "#             # Get S2 titles for the author\n",
    "#             s2_titles = birthdates_df.loc[birthdates_df['author'] == author_name, 'S2 Titles'].iloc[0]\n",
    "#             if isinstance(s2_titles, float):\n",
    "#                 s2_titles = []\n",
    "\n",
    "#             # Perform fuzzy title matching\n",
    "#             title_matched = False\n",
    "#             matched_title = None\n",
    "#             for title_from_viaf in title_list:\n",
    "#                 for s2_title in s2_titles:\n",
    "#                     if fuzz.partial_ratio(s2_title, title_from_viaf) >= 70:\n",
    "#                         title_matched = True\n",
    "#                         matched_title = s2_title\n",
    "#                         break\n",
    "#                 if title_matched:\n",
    "#                     break\n",
    "\n",
    "#             # Append author name, title list, S2 titles, title matched, matched title, and birthdate to search results\n",
    "#             # birthdate = birthdates_df.loc[birthdates_df['author'] == author_name, 'birthdate'].iloc[0]\n",
    "#             search_results.append({'author': author_name, 'record_count': len(records),\n",
    "#                                    'record_enumerated': idx, 'title_list': title_list,\n",
    "#                                    'S2 Titles': s2_titles, 'title_matched': title_matched,\n",
    "#                                    'matched_title': matched_title, 'birthdate': birthdate})\n",
    "            \n",
    "            # Append author name, title list, S2 titles, title matched, matched title, and birthdate to search results\n",
    "            # birthdate = birthdates_df.loc[birthdates_df['author'] == author_name, 'birthdate'].iloc[0]\n",
    "            title_list = df.loc[df['author'] == author_name, 'title_list'].iloc[0]\n",
    "\n",
    "            search_results.append({'author': author_name, 'record_count': len(records),\n",
    "                                   'record_enumerated': idx, 'viaf_title_list': viaf_title_list,\n",
    "                                   'birthdate': birthdate,'title_list':title_list})\n",
    "    except Exception as e:\n",
    "        print(f'Processing Error for {author_name}: {e}')\n",
    "\n",
    "    return search_results\n",
    "\n",
    "# Iterate through each author name in the original DataFrame and perform the search\n",
    "all_search_results = []\n",
    "row_counter = 0  # Counter to limit the number of rows processed\n",
    "\n",
    "for author_name in birthdates_df['author']:\n",
    "    search_results = search_author(author_name)\n",
    "    all_search_results.extend(search_results)\n",
    "    # row_counter += 1\n",
    "    # if row_counter >= 5:  # Adjust the number as needed\n",
    "    #     break\n",
    "\n",
    "# Convert list of dictionaries to DataFrame\n",
    "all_search_results_df = pd.DataFrame(all_search_results)\n",
    "print(all_search_results_df)\n"
   ],
   "id": "71c46f7b9787957a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "result_df = df\n",
    "result_df['VIAF_titlelist'] = result_df['record_enumerated_titles'] \n",
    "result_df['S2_titlelist'] = result_df['S2titles']\n",
    "result_df['average_S2_pubdate'] = result_df['avg_pubdates']\n",
    "result_df['VIAF_birthdate'] = result_df['standard_birthdate']\n",
    "result_df['S2_Author'] = result_df['author']\n",
    "\n",
    "\n",
    "df = result_df"
   ],
   "id": "d8e481e357fae3d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def any_neg(birth2maxdate, birth2mindate):\n",
    "    if birth2maxdate is not None and birth2mindate is not None:\n",
    "        if birth2maxdate < 0 or birth2mindate < 0:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def title_list_len(title_list):\n",
    "    return len(title_list) if isinstance(title_list, list) else 0\n",
    "\n",
    "def author_length(author):\n",
    "    return len(author)"
   ],
   "id": "45b1eeceb2fe81f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# df_test = df.head(10)\n",
    "from ast import literal_eval\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    birth = row['VIAF_birthdate']\n",
    "    pubdates = row['S2_pubdates']\n",
    "    titlelist = row['VIAF_titlelist']\n",
    "    author = row['author']\n",
    "    if pubdates is not None:\n",
    "        print(pubdates)\n",
    "        # print(birth)\n",
    "        pubdates = literal_eval(pubdates)\n",
    "        author = str(row['author'])\n",
    "        birth2maxdate_value = max(pubdates) - int(birth)\n",
    "        birth2mindate_value = min(pubdates)- int(birth)\n",
    "        # any_neg = any_neg(birth2maxdate_value,birth2mindate_value)\n",
    "        title_list_len = len(titlelist)\n",
    "        author_len = len(author)\n",
    "        \n",
    "        # print(birth2maxdate_value)\n",
    "        df.at[idx,'birth2max_value'] = birth2maxdate_value\n",
    "        df.at[idx, 'birth2min_value'] = birth2mindate_value\n",
    "        # df.at[idx, 'any_neg'] = any_neg\n",
    "        df.at[idx, 'title_list_len'] = title_list_len\n",
    "        df.at[idx, 'author_len'] = author_len\n",
    "        # print(birth2maxdate_value)\n",
    "    # else:\n",
    "    #     print(pubdates, birth)\n",
    "    \n"
   ],
   "id": "e9359db7f480e80e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "# Function to compute average publication date\n",
    "def compute_avg_pubdate(pubdates):\n",
    "    try:\n",
    "        # Convert the string representation to an actual tuple\n",
    "        pubdates_tuple = ast.literal_eval(pubdates)\n",
    "        \n",
    "        # Check if it's a tuple of integers\n",
    "        if isinstance(pubdates_tuple, tuple) and all(isinstance(date, int) for date in pubdates_tuple):\n",
    "            return sum(pubdates_tuple) / len(pubdates_tuple)\n",
    "        else:\n",
    "            return None\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "# Apply the function to each row\n",
    "df['avg_pubdate'] = df['S2_pubdates'].apply(lambda x: compute_avg_pubdate(str(x)))\n",
    "\n"
   ],
   "id": "64c392bc4aebb00c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#publication_age\n",
    "df['publication_age'] = df['avg_pubdate'] - df['VIAF_birthdate']"
   ],
   "id": "6487ff54f323a55d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df['publication_age'] = pd.to_numeric(df['publication_age'], errors='coerce')\n",
   "id": "574da93ce6191d8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = df.fillna(0)\n",
   "id": "743474d03b2775cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['status'] = \"\"\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row['publication_age'] < 0:\n",
    "        df.at[idx,'status'] = 'not_born'\n",
    "    elif row['publication_age'] > 100:\n",
    "        df.at[idx,'status'] = 'zombie'\n",
    "    elif row['publication_age'] < 9:\n",
    "        df.at[idx,'status'] = 'toddler'\n",
    "    else:\n",
    "        df.at[idx,'status'] = '0'"
   ],
   "id": "a5fecadb001e9605"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['status'] = df['status'].str.replace('zombie', '1')\n",
    "df['status'] = df['status'].fillna('0')\n",
    "df['status'] = df['status'].str.replace('not_born', '2')\n",
    "df['status'] = df['status'].str.replace('toddler', '3')\n"
   ],
   "id": "6adaf255a733c6cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now add VIAF and S2 titlelist embeddings",
   "id": "81f3d6d7460591f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#embeddings\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to generate embeddings for a list of titles\n",
    "def get_embeddings(titles_list):\n",
    "    if isinstance(titles_list, str):\n",
    "        return model.encode(titles_list)\n",
    "    else:\n",
    "        return model.encode('')\n",
    "\n",
    "# Generate embeddings for each row in the DataFrame\n",
    "df['S2_embeddings'] = df['S2titles'].apply(get_embeddings)\n",
    "df['VIAF_embeddings'] = df['VIAF_titlelist'].apply(get_embeddings)\n",
    "\n",
    "\n",
    "#was taking forever to run, so I'm leaving this piece out for now to make sure everything else works!"
   ],
   "id": "54b5cff3283e79cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "df['S2_embeddings'] = df['S2_embeddings'].apply(np.mean)\n",
    "df['VIAF_embeddings'] = df['VIAF_embeddings'].apply(np.mean)"
   ],
   "id": "2017ef2415028292"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#get overlapping words and overlapping lemmas:\n",
    "# Function to lemmatize text and return a set of lemmatized words\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return set(token.lemma_ for token in doc if not token.is_stop and not token.is_punct)\n",
    "\n",
    "# Function to calculate lemma overlap and return the overlapping lemmas\n",
    "def calculate_lemma_overlap(text1, text2):\n",
    "    text1 = str(text1)\n",
    "    text2 = str(text2)\n",
    "    lemmas1 = lemmatize_text(text1)\n",
    "    lemmas2 = lemmatize_text(text2)\n",
    "    overlap = lemmas1.intersection(lemmas2)\n",
    "    return len(overlap), overlap\n",
    "\n",
    "# # Apply the function to compute lemma overlap for each row in the DataFrame\n",
    "# df[['lemma_overlap', 'overlapping_lemmas']] = df.apply(\n",
    "#     lambda row: pd.Series(calculate_lemma_overlap(row['VIAF_titlelist'], row['S2_titlelist'])), \n",
    "#     axis=1)\n",
    "df[['lemma_overlap', 'overlapping_lemmas']] = df.apply(\n",
    "    lambda row: pd.Series(calculate_lemma_overlap(row['VIAF_titlelist'], row['S2_titlelist'])), \n",
    "    axis=1)\n"
   ],
   "id": "b2c0b407e93bef3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#add overlaps\n",
    "\n",
    "#add word overlap as a new feature\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords from NLTK if you haven't already\n",
    "nltk.download('stopwords')\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to create bag of words after removing stop words\n",
    "def bag_of_words(title):\n",
    "    words = title.lower().split()\n",
    "    return set([word for word in words if word not in stop_words])\n",
    "\n",
    "# Function to find and count word overlap\n",
    "def find_word_overlap(row):\n",
    "    try:\n",
    "        v_list = ast.literal_eval(row['VIAF_titlelist'])\n",
    "        s_list = ast.literal_eval(row['S2_titlelist'])\n",
    "        if isinstance(v_list, list):\n",
    "\n",
    "            for title in v_list:\n",
    "                title = str(title)\n",
    "        if isinstance(s_list, list):\n",
    "\n",
    "            for title in s_list:\n",
    "                title = str(title)\n",
    "        if isinstance(v_list, list):\n",
    "\n",
    "            v_bag = bag_of_words(v_list) \n",
    "        if isinstance(s_list, list):\n",
    "\n",
    "            s_bag = bag_of_words(s_list) \n",
    "\n",
    "        overlap = v_bag & s_bag\n",
    "    except:\n",
    "        overlap = ''\n",
    "    return pd.Series([len(overlap), list(overlap)])\n",
    "  \n",
    "# Apply the function to each row and create two new columns\n",
    "df[['word_overlap_count', 'overlapping_words']] = df.apply(find_word_overlap, axis=1)"
   ],
   "id": "d757595f5c94b1f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (precision_recall_curve, roc_auc_score,\n",
    "                             confusion_matrix, ConfusionMatrixDisplay,\n",
    "                             classification_report, accuracy_score)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X = df.drop(columns=['author','VIAF_titlelist','S2_titlelist','overlapping_words','selected_birthyear','overlapping_lemmas','title_list','record_enumerated_titles','S2Titles','S2titles','matched_title_list','common_words','notes','normalized_author','S2_Titlelist','S2_Author','S2_pubdates','mean_embedding','matched_title?','match?','match'])  # Drop the label and metadata columns\n",
    "y = df['match?']\n",
    "\n",
    "# Optional: If you want to split into training and test sets first\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ],
   "id": "c60e5a35a989477c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "model = RandomForestClassifier()\n",
    "y_pred_proba = cross_val_predict(model, X, y, cv=5, method='predict_proba')\n",
    "\n",
    "# Store predictions with original indices\n",
    "predictions_df = pd.DataFrame(y_pred_proba, index=original_indices, columns=['Class_0_Proba', 'Class_1_Proba'])\n",
    "\n",
    "# Merge the metadata back\n",
    "results_df = pd.concat([original_metadata, predictions_df], axis=1)\n",
    "results_df['True_Label'] = y.loc[results_df.index]\n",
    "\n",
    "# Now you have a DataFrame with metadata and predictions\n",
    "print(results_df.head())"
   ],
   "id": "2dc82746d0cd1676"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform cross-validation to get predicted probabilities\n",
    "y_pred_proba = cross_val_predict(model, X, y, cv=5, method='predict_proba')\n",
    "\n",
    "# Store predictions with original indices\n",
    "predictions_df = pd.DataFrame(y_pred_proba, index=original_indices, columns=['Class_0_Proba', 'Class_1_Proba'])\n",
    "\n",
    "# Merge the metadata back\n",
    "results_df = pd.concat([original_metadata, predictions_df], axis=1)\n",
    "results_df['True_Label'] = y.loc[results_df.index]\n",
    "\n",
    "# Define the threshold for decision\n",
    "threshold = 0.5  # You can change this value based on your needs\n",
    "\n",
    "# Compute class decisions based on the threshold\n",
    "results_df['Class_Decision'] = (results_df['Class_1_Proba'] > threshold).astype(int)\n",
    "\n",
    "# Now you have a DataFrame with metadata, predictions, and class decisions\n",
    "print(results_df.head())\n"
   ],
   "id": "fe3dcdade4cefc4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#find the optimal threshold\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform cross-validation to get predicted probabilities\n",
    "y_pred_proba = cross_val_predict(model, X, y, cv=5, method='predict_proba')\n",
    "\n",
    "# Store predictions with original indices\n",
    "predictions_df = pd.DataFrame(y_pred_proba, index=original_indices, columns=['Class_0_Proba', 'Class_1_Proba'])\n",
    "\n",
    "# Define the threshold for decision\n",
    "threshold = 0.8  # You can change this value based on your needs\n",
    "\n",
    "# Compute class decisions based on the threshold\n",
    "predictions_df['Class_Decision'] = (predictions_df['Class_1_Proba'] > threshold).astype(int)\n",
    "\n",
    "# Merge the metadata back\n",
    "results_df = pd.concat([original_metadata, predictions_df], axis=1)\n",
    "results_df['True_Label'] = y.loc[results_df.index]\n",
    "\n",
    "# Generate class predictions\n",
    "y_pred = (y_pred_proba[:, 1] > threshold).astype(int)  # Use the same threshold for predictions\n",
    "\n",
    "# Print the DataFrame with metadata, predictions, and class decisions\n",
    "print(results_df.head())\n",
    "\n",
    "# Generate and print the classification report\n",
    "report = classification_report(y, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ],
   "id": "c6aa902057bbf269"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
