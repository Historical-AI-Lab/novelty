{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUilkIY6c8RL"
   },
   "outputs": [],
   "source": [
    "# def get_last_names():\n",
    "#   lowercase_last_names = []\n",
    "#   for author in cited_authors:     # you'll need to .split() each author and take only the last word — we get first names as well as last but you only want to check the last\n",
    "#     lowercase_last_names.append(author.lower())\n",
    "#   return lowercase_last_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing github changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XP6GdgP8dq4s"
   },
   "outputs": [],
   "source": [
    "def theres_an_author_match(citing_set, lowercase_last_names):\n",
    "  flatten_set_of_tuples()    # you need to put citing_set in the parens like all_words_in_chunk = flatten_set_of_tuples(citing_set)\n",
    "  author_match = False          # you could set this initially to False, and set it to True if match is found\n",
    "  #check if author name matches any tokens in any of 3 grams\n",
    "  for item in list:      # ah — you can simplify the nested loops by just saying if name in all_words_in_chunk: where you currently say if item == name:\n",
    "    for name in lowercase_last_names:\n",
    "      #if item == 'Anon'\n",
    "\n",
    "      if item == name:\n",
    "        #author_match.append(item) # just say author_match = True?  we don't care how many matches there are, so break?\n",
    "        author_match = True\n",
    "\n",
    "    #question -- would it be better to use some sort of similarity score here for last names instead of an exact match?\n",
    "\n",
    "    # I think exact match is okay since we're only checking last name. Fuzzy match becomes necessary when there are first names OR initials etc etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZYoD5wcfYui"
   },
   "outputs": [],
   "source": [
    "def flatten_set_of_tuples(tuples):\n",
    "    word_list = []\n",
    "    for tup in tuples:\n",
    "        word_list.extend(tup)\n",
    "    return word_list        # yep I think this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-9j2vClfvFZ"
   },
   "outputs": [],
   "source": [
    "def count_repeats():\n",
    "  two_or_more = []\n",
    "  three_or_more = []\n",
    "  for word in all_words_that_overlap:\n",
    "    word_counter = 0\n",
    "    if word in all_words_that_overlap:\n",
    "      word_counter += 1\n",
    "      if word_counter >= 3:\n",
    "        three_or_more.append(word)\n",
    "      if word_counter >= 2:\n",
    "        two_or_more.append(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mSr9AdDiWkV"
   },
   "outputs": [],
   "source": [
    "def if_any_had_quotes():\n",
    "  for author in cited_authors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESKrK-Sisj_g"
   },
   "outputs": [],
   "source": [
    "#input is a list of names, output is just the last name lowercased\n",
    "def lowercaseLastNames(cited_authors):\n",
    "    lowercase_last_names=[]\n",
    "    for name in cited_authors:\n",
    "        nameParts = name.split()\n",
    "        #add literal_eval\n",
    "        lowercase_last_names.append(nameParts[-1].lower())\n",
    "    return lowercase_last_names\n",
    "\n",
    "#Test\n",
    "cited_authors = [\"Simon P. Anderson\", \"Mark Twain\", \"Ted Overwood\", \"John Splitsy-Hyphen\"] # love splitsy-hyphen!! :)\n",
    "lowercaseLastNames(cited_authors)         # It's a good idea to run tests like this for each of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8rHVRBggyy7"
   },
   "outputs": [],
   "source": [
    "#Another way to look at count_repeats:\n",
    "\n",
    "def count_repeats(all_words_in_overlap):\n",
    "    words = all_words_in_overlap.split()\n",
    "    word_counts = Counter(words)\n",
    "    count_first_condition = any(count >= 3 for count in word_counts.values())\n",
    "    count_second_condition = sum(count >= 2 for count in word_counts.values()) >= 4\n",
    "    return count_first_condition and count_second_condition     # nice technique, separating the conditions cdproj\n",
    "    allows us to change criteria later if we need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMaAozFvg6LX",
    "outputId": "6e8b0856-ff1c-471f-f066-2c6efb94bc08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', 'economic', 'studies', 'review', 'of', 'economic']\n"
     ]
    }
   ],
   "source": [
    "#Two versions of tuple flattening: only need the first one.\n",
    "#Don't need to worry about word order\n",
    "\n",
    "def flatten_set_of_tuples(overlapping3grams):\n",
    "    flattened_set = []\n",
    "    for tup in overlapping3grams:\n",
    "        for word in tup:\n",
    "            if isinstance(word, (str)):\n",
    "                flattened_set.append(word)\n",
    "    return(flattened_set)\n",
    "\n",
    "'''def flatten_set_of_tuples2(overlapping3grams):\n",
    "    sequence = [i[0] for i in overlapping3grams[:-1]]\n",
    "    sequence += overlapping3grams[-1]\n",
    "    return(sequence)'''\n",
    "\n",
    "# Example usage:\n",
    "tuples_set3 = {(\"review\", \"of\", \"economic\"), (\"of\", \"economic\", \"studies\")}\n",
    "print(flatten_set_of_tuples(tuples_set3))\n",
    "#print(flatten_set_of_tuples2(tuples_set3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U46A9YrTvRiD"
   },
   "outputs": [],
   "source": [
    "#function to find words with quotation marks (single or double) attached to them. We'll need this to compare later with overlapping words from the 3grams.\n",
    "#It's not perfect because it still picks up apostrophes from conjunctions...\n",
    "\n",
    "def find_quoted_words(text):\n",
    "    tokenizedSent = text.split()\n",
    "    had_quotes = [i for i in tokenizedSent if \"\\\"\" in i]\n",
    "    had_quotes += [i for i in tokenizedSent if \"\\'\" in i]\n",
    "    had_quotes = [word.strip('\\\"\\'.') for word in had_quotes]\n",
    "    return had_quotes\n",
    "\n",
    "# Example\n",
    "example_text = 'We agree with \\'everything\\' except Hubble\\'s claim that \"what we don\\'t know cannot kill us.\"'\n",
    "find_quoted_words(example_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeCa_hj1y_Et"
   },
   "outputs": [],
   "source": [
    "#matching between all the words we found that had overlap and the words that had quotation marks attached to them:\n",
    "#Question: not every instance of a word had quotation marks attached to it. We need to check whether the correct \"instances\" of each word are matching..\n",
    "\n",
    "def did_any_have_quotes(all_words_in_overlap, had_quotes):\n",
    "    quoted_overlap_words = [i for i in all_words_in_overlap if i in had_quotes]\n",
    "    return quoted_overlap_words\n",
    "\n",
    "#Test:\n",
    "overlapping_words = ['what','is','the','economic','review','doing','about','this']\n",
    "quoted_words = ['what', 'us', 'everything', \"Hubble's\", \"don't\"]\n",
    "did_any_have_quotes(overlapping_words, quoted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcAyY45CdT8V"
   },
   "outputs": [],
   "source": [
    "def get_forbidden_combos(cited3grams, citing3grams, had_quotes, cited_authors):\n",
    "forbidden = []\n",
    "for idx, citing_set in enumerate(citing3grams):\n",
    "  if theres_an_author_match(citing_set, lowercase_last_names):   # you can write a function to check this\n",
    "        forbidden.append(idx)\n",
    "        continue      # if any author names match any tokens in any of the 3grams, this citing chunk is forbidden\n",
    "                      # and we can proceed to the next\n",
    "                      # otherwise, we need to look for quotes\n",
    "    for cited_set in cited3grams:\n",
    "            # Then the next thing is, we don't have to compare the 3grams individually.\n",
    "      # The point of having a set is you can do this ...\n",
    "      overlapping3grams = cited_set.intersection(citing_set)    # .intersection() finds all the matches\n",
    "            if len(overlapping3grams) < 4:      # a shared 6-word sequence will create I think four shared 3-grams\n",
    "          continue                        # if we don't have four, there cannot be a shared 6-word sequence\n",
    "                                          # so go to the next cited_set\n",
    "      all_words_in_overlap = flatten_set_of_tuples(overlapping3grams)\n",
    "                             # I'll imagine you've written a function that turns\n",
    "                             # a set of tuples into a list of all the words in the tuples\n",
    "                             # e.g [(\"review\", \"of\", \"economic\"), (\"of\", \"economic\", \"studies\")]\n",
    "                             # ==> [\"review\", \"of\", \"economic\", \"of\", \"economic\", \"studies\"]\n",
    "            enough_repeats = count_repeats(all_words_in_overlap)\n",
    "                        # this is a function that checks to see if at least two words appear 3 or more times\n",
    "                        # and at least four words appear 2 or more times; this must be true if there's a\n",
    "                        # shared sequence for reasons explained at the end of this document\n",
    "                        # The function should return True or False\n",
    "            anything_had_quotes = did_any_have_quotes(all_words_in_overlap, had_quotes)  # left as exercise\n",
    "            if enough_repeats and anything_had_quotes:\n",
    "                forbidden.append(idx)\n",
    "                break      # we don't need to compare it to any other chunks of the cited document\n",
    "                           # because one match is enough to condemn it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEcDmdLQXyuN"
   },
   "outputs": [],
   "source": [
    "# def get_forbidden_combos(cited3grams, citing3grams, had_quotes, cited_authors):\n",
    "#   #INPUTS\n",
    "#   cited_file = #list of chunks (each chunk is a set of 3 grams) from cited file\n",
    "#   citing_file = #list of 3 gram chunks from citing file\n",
    "#   words_set_list = #citing file chunk represented as a set of indivual words (quoted, but no longer has quotes attached)\n",
    "#   cited_authors = #authors cited by the citing file?\n",
    "#   #Function's Mission\n",
    "#   #return a list of indexes from the citing file, of chunks that contained either a last name or at least 6 words in sequence shared with any chunk (and one of those words had quotes attached)\n",
    "#   index_list = []\n",
    "#   for items in cited_file:\n",
    "#     for grams in citing_file:\n",
    "#       if item == gram:\n",
    "#         for index in cited_file:\n",
    "#           index_list.append(index) #should identify any matches between the 3 gram sets? -- but I think it is supposed to be 6 words instead...\n",
    "#       for name in cited_authors:\n",
    "#         if name == item:\n",
    "#           index_list.append(index)\n",
    "#   return(index_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdNYLMNtNHk2"
   },
   "outputs": [],
   "source": [
    "#read in the chunks: from the campus cluster, text files (don't actually have to do this? write function assumed passed 2 articles you want to compare, he will write framework to handle metadata)\n",
    "#spreadsheet with semantic scholar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# convert each chunk into dictionary (or set) of three grams, using NLTK\n",
    "#moving window of 3 words, set of 3 grams\n",
    "#one article in the future and all its chunks are sets of three grams\n",
    "#compare using setname.intersection other setname\n",
    "#need to have at least 4 in common\n",
    "#most chunks will fail that\n",
    "\n",
    "\n",
    "#set of 3 grams in the entire article, not just the chunk, if not at least 5 of the 3 grams in common with whole article\n",
    "#then for those in common, do the indivual chunk comparisons\n",
    "#look at the set of 3 grams shared b/w the chunks, something needs to be true about the words that compose those, if they overlapped/part of the same string\n",
    "#how many words are shared between those 3 grams\n",
    "#4 words that occur 3 times within the set\n",
    "#count #from collections import counter, like a dictionary but starts everything at 0, increment the count\n",
    "\n",
    "#put all strings together, make counter list of words\n",
    "#find 3 words that are repeated 3 times\n",
    "\n",
    "#delta folder with the chunks\n",
    "\n",
    "\n",
    "#identify the DOI, each is a seperate line of the file, no extraneous new lines\n",
    "#each line is a chunk, split on the tab and take everything after the chunk\n",
    "\n",
    "#bbiq\n",
    "#embedding code folder\n",
    "#results of the scripts\n",
    "#subfolder called chunks\n",
    "#last part of DOI\n",
    "#list of chunks\n",
    "#chunk #, tab, up to 512 tokens\n",
    "\n",
    "#start with json to see cited\n",
    "#spreadsheet with titles and author and year, (metadata, 20 years in future etc and also listed as citing this\n",
    "#then citations json file in email and find the subset\n",
    "#then read in the chunks, each article is a seperate, named with semantic scholar ID\n",
    "#semantic scholar ID.txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsnZ1S4QQbff"
   },
   "outputs": [],
   "source": [
    "#for one cited article and its metadata and the list of file IDs for articles that cite that article\n",
    "#read in/define article as its chunks?\n",
    "article_metadata = []\n",
    "#this is in the github repo, when we search semantic scholar for list of citations, we output spreadsheet with author, title etc, BUT also need JSON lines file where each article represented for a specific JSON with the specific articles that cite it\n",
    "#prob represented by a semantic scholar ID, but have to\n",
    "#JSONS from semantic scholar\n",
    "article_cited = []\n",
    "#for each chunk, if more than 4 of those three word phrases are shared between 2 articles = overlap phrase\n",
    "#if more than 3 and shared phrases share words then = overlap phrase\n",
    "#when there's an overlap phrase, we will exclude that chunk from our final output (probably not at this point though)\n",
    "#store the ID correlated with the chunk that has a overlap phrase\n",
    "#cited article and list of IDs that cite it\n",
    "#create a dictionary of how many chunks are in each file (aka list of chunk IDs in each article)\n",
    "#function returns the \"forbidden\" chunks we need to exclude/that cannot be compared to our article\n",
    "\n",
    "\n",
    "#lopp thru\n",
    "#if author name in chunk, add this chunk ID to list of forbidden"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
